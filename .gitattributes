# *.pth filter=lfs diff=lfs merge=lfs -text
import numpy as np
import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import _stop_words

reddit_df = pd.read_csv("Cleaned_dataset.csv")

reddit_df.head()

nltk.download("vader_lexicon")
sia = SentimentIntensityAnalyzer()

reddit_df["Combined_Text"] = reddit_df["Cleaned_Title"]  + " " + reddit_df["Cleaned_Content"]
def get_vader_sentiment(text):
    score = sia.polarity_scores(text)["compound"]
    if score > 0.2:
        return "Positive"
    elif score < -0.2:
        return "Negative"
    else:
        return "Neutral"
reddit_df["Sentiment"] = reddit_df["Combined_Text"].apply(get_vader_sentiment)
tfidf = TfidfVectorizer(max_features=1000, stop_words="english", ngram_range=(1,2))
tfidf_matrix = tfidf.fit_transform(reddit_df["Combined_Text"])
feature_names = tfidf.get_feature_names_out()
tfidf_scores = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
Important_Terms = tfidf_scores.mean().sort_values(ascending=False)[:20].index.tolist()
def classify_risk(text, sentiment):
    text = text.lower()
    high_risk_terms = ["suicide","suicidal", "kill myself", "end it", "i give up", "overdose", "goodbye", "can't go on", "commit suicide", "die", "kill", "attempt","ending life", "end life"]
    moderate_risk_terms = ["depressed", "addiction", "rehab", "cannabis", "molly", "heroin", "marijuana", "fentanyl", "pot", "hopeless", "numb", "empty", "need help", "worthless", "anxious", "panic attack", "im high", "drug"]
    if sentiment == "Negative" or sentiment=="Neutral":
        if any(term in text for term in high_risk_terms):
            return "High-Risk"
        elif any(term in text for term in moderate_risk_terms):
            return "Moderate Concern"
    return "Low Concern"
reddit_df["Risk_Level"] = reddit_df.apply(lambda row: classify_risk(row["Combined_Text"], row["Sentiment"]), axis=1)

sentiment_counts = reddit_df["Sentiment"].value_counts()
plt.figure(figsize=(8, 5))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette={"Positive": "green", "Neutral": "gray", "Negative": "red"})
plt.xlabel("Sentiment", fontsize=12)
plt.ylabel("Number of Posts", fontsize=12)
plt.title("Distribution of Sentiments in Reddit Posts", fontsize=14)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
for index, value in enumerate(sentiment_counts.values):
    plt.text(index, value + 10, str(value), ha="center", fontsize=12, fontweight="bold")
plt.show()

risk_counts = reddit_df["Risk_Level"].value_counts()
plt.figure(figsize=(8, 5))
sns.barplot(x=risk_counts.index, y=risk_counts.values, palette={"Low Concern": "green", "Moderate Concern": "gray", "High-Risk": "red"})
plt.xlabel("Risk", fontsize=12)
plt.ylabel("Number of Posts", fontsize=12)
plt.title("Distribution of Risk in Reddit Posts", fontsize=14)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
for index, value in enumerate(risk_counts.values):
    plt.text(index, value + 10, str(value), ha="center", fontsize=12, fontweight="bold")
plt.show()

temp = reddit_df["Risk_Level"] == "Moderate Concern"
temp = reddit_df[temp]
temp = temp["Combined_Text"]
temp

pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.expand_frame_repr", False)
pd.set_option("display.max_colwidth", None)

negative_texts = reddit_df[reddit_df["Sentiment"].isin(["Negative", "Neutral"])]["Combined_Text"]
high_risk_texts = reddit_df[reddit_df["Risk_Level"] == "High-Risk"]["Combined_Text"]
moderate_risk_texts = reddit_df[reddit_df["Risk_Level"]=="Moderate Concern"]["Combined_Text"]



tfidf_high = TfidfVectorizer(
    max_features=500,
    stop_words="english",
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.8
)
tfidf_moderate = TfidfVectorizer(
    max_features=500,
    stop_words="english",
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.8
)
tfidf_high.fit(negative_texts)
tfidf_moderate.fit(negative_texts)
tfidf_matrix_high = tfidf_high.transform(high_risk_texts)
tfidf_matrix_moderate = tfidf_moderate.transform(moderate_risk_texts)
feature_names_high = tfidf_high.get_feature_names_out()
feature_names_moderate = tfidf_moderate.get_feature_names_out()
tfidf_scores_high = pd.DataFrame(tfidf_matrix_high.toarray(), columns=feature_names_high)
tfidf_scores_moderate = pd.DataFrame(tfidf_matrix_moderate.toarray(), columns=feature_names_moderate)
important_terms_high = tfidf_scores_high.mean().sort_values(ascending=False)[:30].index.tolist()
important_terms_moderate = tfidf_scores_moderate.mean().sort_values(ascending=False)[:30].index.tolist()
print(f"High risk terms:{important_terms_high}\n")
print(f"Moderate risk terms:{important_terms_moderate}")


generic_words=[]
for i in important_terms_high:
  if i in Important_Terms:
    generic_words.append(i)
for i in important_terms_moderate:
  if i in Important_Terms:
    generic_words.append(i)
generic_words

custom_stopwords = set(_stop_words.ENGLISH_STOP_WORDS).union(generic_words)
custom_stopwords = list(custom_stopwords)
tfidf_new_high = TfidfVectorizer(
    max_features=500,
    stop_words=custom_stopwords,
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.8 )
tfidf_new_moderate = TfidfVectorizer(
    max_features=500,
    stop_words=custom_stopwords,
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.8 )
tfidf_new_high.fit(negative_texts)
tfidf_new_moderate.fit(negative_texts)
tfidf_matrix_new_high = tfidf_new_high.transform(high_risk_texts)
tfidf_matrix_new_moderate = tfidf_new_moderate.transform(moderate_risk_texts)
feature_names_new_high = tfidf_new_high.get_feature_names_out()
feature_names_new_moderate = tfidf_new_moderate.get_feature_names_out()
tfidf_scores_new_high = pd.DataFrame(tfidf_matrix_new_high.toarray(), columns=feature_names_new_high)
tfidf_scores_new_moderate = pd.DataFrame(tfidf_matrix_new_moderate.toarray(), columns=feature_names_new_moderate)
important_terms_new_high = tfidf_scores_new_high.mean().sort_values(ascending=False)[:150].index.tolist()
important_terms_new_moderate = tfidf_scores_new_moderate.mean().sort_values(ascending=False)[:150].index.tolist()
print("Refined High-Risk Terms:", important_terms_new_high)
print("\n")
print("Refined Moderate-Risk Terms:", important_terms_new_moderate)

distinct_high_risk_terms = list(set(important_terms_new_high) - set(important_terms_new_moderate))
print("Distinct High-Risk Terms:", distinct_high_risk_terms)

distinct_moderate_risk_terms = list(set(important_terms_new_moderate) - set(important_terms_new_high))
print("Distinct moderate-Risk Terms:", distinct_moderate_risk_terms)

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
def filter_generic_words(word_list, additional_stopwords=None):
    """
    Removes generic words (stopwords, common Reddit terms, etc.) from a given word list.
    Args:
        word_list (list): List of words to filter.
        additional_stopwords (set): Optional additional words to remove.
    Returns:
        list: Filtered list with generic words removed.
    """
    reddit_stopwords = {"post", "deleted", "edit", "comments", "account", "people", "really",
                        "like", "just", "know", "think", "said", "man", "going","ok","tells", "asked","us","want","human", "im", "dont"}
    stopword_set = ENGLISH_STOP_WORDS.union(reddit_stopwords)
    if additional_stopwords:
        stopword_set = stopword_set.union(additional_stopwords)
    filtered_words = [word for word in word_list if word.lower() not in stopword_set]
    return filtered_words

negative_texts

from sklearn.feature_selection import chi2
import numpy as np
tfidf_shared = TfidfVectorizer(
    max_features=3000,
    stop_words="english",
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.8
)
negative_df = reddit_df[reddit_df["Sentiment"].isin(["Negative", "Neutral"])]
X_combined = tfidf_shared.fit_transform(negative_texts)
y_combined = (negative_df["Risk_Level"] == "High-Risk").astype(int)
chi2_scores, p_values = chi2(X_combined, y_combined)
feature_names = tfidf_shared.get_feature_names_out()
top_chi2_indices = np.argsort(chi2_scores)[::-1][:60]
top_chi2_words = feature_names[top_chi2_indices]
significant_words = [word for i, word in enumerate(top_chi2_words) if p_values[top_chi2_indices[i]] < 0.25]
print("**Top High-Risk Indicator Words by Chi-Square Test (Filtered):**")
print(filter_generic_words(significant_words, additional_stopwords=None))



tfidf_shared_moderate = TfidfVectorizer(
    max_features=3000,
    stop_words="english",
    ngram_range=(1, 2),
    min_df=0.0,
    max_df=0.8
)
X_combined_moderate = tfidf_shared_moderate.fit_transform(reddit_df["Combined_Text"])
y_combined_moderate = (reddit_df["Risk_Level"] == "Moderate Concern").astype(int)
chi2_scores_moderate, p_values_moderate = chi2(X_combined_moderate, y_combined_moderate)
feature_names_moderate = tfidf_shared_moderate.get_feature_names_out()
top_chi2_indices_moderate = np.argsort(chi2_scores_moderate)[::-1][:60]
top_chi2_words_moderate = feature_names_moderate[top_chi2_indices_moderate]
significant_words_moderate = [word for i, word in enumerate(top_chi2_words_moderate) if p_values_moderate[top_chi2_indices_moderate[i]] < 0.25]
print("**Top Moderate-Risk Indicator Words by Chi-Square Test (Filtered):**")
print(filter_generic_words(significant_words_moderate, additional_stopwords=significant_words))

from gensim.models import Word2Vec
import nltk
nltk.download('punkt')
negative_texts["Tokenized"] = negative_texts.apply(lambda x: nltk.word_tokenize(x) if isinstance(x, str) else [])
moderate_risk_texts["Tokenized"] = moderate_risk_texts.apply(lambda x: nltk.word_tokenize(x) if isinstance(x, str) else [])
w2v_model = Word2Vec(sentences=negative_texts["Tokenized"], vector_size=100, window=5, min_count=3, workers=4)
w2v_model_moderate = Word2Vec(sentences=moderate_risk_texts["Tokenized"], vector_size=100, window=5, min_count=3, workers=4)
seed_words = filter_generic_words(significant_words)
seed_words_moderate = filter_generic_words(significant_words_moderate)
expanded_words = {}
expanded_words_moderate= {}
for word in seed_words:
    if word in w2v_model.wv:
        expanded_words[word] = filter_generic_words([w[0] for w in w2v_model.wv.most_similar(word, topn=10)], additional_stopwords=["simply","bring","knowing","continue","became","highschool","began","full","university","older","obviously","however","put","instead","check","name","true","writing","hobbies","fully","kinda","idk","point","normal","including","eventually"])
for word in seed_words_moderate:
    if word in w2v_model_moderate.wv:
        expanded_words_moderate[word] = filter_generic_words([w[0] for w in w2v_model_moderate.wv.most_similar(word, topn=10)], additional_stopwords=["simply","bring","knowing","continue","became","highschool","began","full","university","older","obviously","however","put","instead","check","name","true","writing","hobbies","fully","kinda","idk","point","normal","including","eventually"])
print("Expanded High-Risk Words:", expanded_words)
print("\n")
print("Expanded moderate-Risk Words:", expanded_words_moderate)

results = {}
for key_word, words in expanded_words.items():
    valid_words = [word for word in words if word in feature_names_new_high]
    if not valid_words:
        continue
    high_risk_scores = tfidf_scores_new_high[valid_words].mean()
    moderate_risk_scores = tfidf_scores_new_moderate[valid_words].mean()
    score_difference = (high_risk_scores - moderate_risk_scores).sort_values(ascending=False)
    X_high = tfidf_matrix_new_high.toarray()
    X_moderate = tfidf_matrix_new_moderate.toarray()
    y_high = np.ones(X_high.shape[0])
    y_moderate = np.zeros(X_moderate.shape[0])
    X_combined = np.vstack((X_high, X_moderate))
    y_combined = np.hstack((y_high, y_moderate))
    chi2_scores, p_values = chi2(X_combined, y_combined)
    word_chi2_scores = {word: chi2_scores[np.where(feature_names_new_high == word)[0][0]]
                        for word in valid_words}
    results[key_word] = {
        "valid_words": valid_words,
        "score_difference": score_difference.to_dict(),
        "chi2_scores": word_chi2_scores
    }
df_results = pd.DataFrame.from_dict(results, orient='index')
print(df_results)

df_results.shape
df_results.iloc[10]

chi2_values_00000 =df_results["chi2_scores"].apply(lambda x: np.mean(list(x.values())) if x else 0)
print(chi2_values_00000)

threshold_chi2 = 1
for key in df_results.index:
    valid_words = df_results.at[key, "valid_words"]
    chi2_scores = df_results.at[key, "chi2_scores"]
    score_diff = df_results.at[key, "score_difference"]
    filtered_words = [
        word for word in valid_words
        if chi2_scores.get(word, 0) >= threshold_chi2 and score_diff.get(word, 0) > 0
    ]
    filtered_chi2 = {word: chi2_scores[word] for word in filtered_words}
    filtered_score_diff = {word: score_diff[word] for word in filtered_words}
    df_results.at[key, "valid_words"] = filtered_words
    df_results.at[key, "chi2_scores"] = filtered_chi2
    df_results.at[key, "score_difference"] = filtered_score_diff

df_results

chi2_values_ = df_results["chi2_scores"].apply(lambda x: np.mean(list(x.values())) if x else 0)
print(chi2_values_)

filtered_chi2_values = chi2_values_[chi2_values_ > 0]
print(filtered_chi2_values)

filtered_rows = df_results.loc[chi2_values_ > 0]
High_risk_valid_words_dict = {}
for key in filtered_rows.index:
    valid_words = df_results.at[key, "valid_words"]
    High_risk_valid_words_dict[key] = valid_words
print(High_risk_valid_words_dict)

results_moderate = {}
for key_word, words in expanded_words_moderate.items():
    valid_words = [word for word in words if word in feature_names_new_moderate]
    if not valid_words:
        continue
    high_risk_scores = tfidf_scores_new_high[valid_words].mean()
    moderate_risk_scores = tfidf_scores_new_moderate[valid_words].mean()
    score_difference = (moderate_risk_scores - high_risk_scores).sort_values(ascending=False)
    X_high = tfidf_matrix_new_high.toarray()
    X_moderate = tfidf_matrix_new_moderate.toarray()
    y_high = np.zeros(X_high.shape[0])
    y_moderate = np.ones(X_moderate.shape[0])
    X_combined = np.vstack((X_moderate, X_high))
    y_combined = np.hstack((y_moderate, y_high))
    chi2_scores, p_values = chi2(X_combined, y_combined)
    word_chi2_scores = {word: chi2_scores[np.where(feature_names_new_moderate == word)[0][0]]
                        for word in valid_words}
    results_moderate[key_word] = {
        "valid_words": valid_words,
        "score_difference": score_difference.to_dict(),
        "chi2_scores": word_chi2_scores
    }
df_results_moderate = pd.DataFrame.from_dict(results_moderate, orient='index')
print(df_results_moderate)

df_results_moderate.shape
df_results_moderate.iloc[10]

chi2_values_000 =df_results_moderate["chi2_scores"].apply(lambda x: np.mean(list(x.values())) if x else 0)
print(chi2_values_000)

threshold_chi2 = 1
for key in df_results_moderate.index:
    valid_words = df_results_moderate.at[key, "valid_words"]
    chi2_scores = df_results_moderate.at[key, "chi2_scores"]
    score_diff = df_results_moderate.at[key, "score_difference"]
    filtered_words = [
        word for word in valid_words
        if chi2_scores.get(word, 0) >= threshold_chi2 and score_diff.get(word, 0) > 0
    ]
    filtered_chi2 = {word: chi2_scores[word] for word in filtered_words}
    filtered_score_diff = {word: score_diff[word] for word in filtered_words}
    df_results_moderate.at[key, "valid_words"] = filtered_words
    df_results_moderate.at[key, "chi2_scores"] = filtered_chi2
    df_results_moderate.at[key, "score_difference"] = filtered_score_diff

df_results_moderate

Moderate_Risk_words = df_results_moderate.index.tolist()
Moderate_Risk_words

Moderate_Risk_Words_Final = ['panic','depressed','anxious','addiction','attacks','numb','ssris','thoughts','attack','derealization','triggered',
 'love','visa','anxiety','friend','best','live','lost','overwhelmed','test','realizing','care','highschool','stressed','life',
 'truly']

tfidf_vectorizer_bg_high = TfidfVectorizer(ngram_range=(2,3), max_features=100)
tfidf_matrix_bg_high = tfidf_vectorizer_bg_high.fit_transform(high_risk_texts)
feature_names_bg_high = tfidf_vectorizer_bg_high.get_feature_names_out()
tfidf_scores_bg_high = pd.DataFrame(tfidf_matrix_bg_high.toarray(), columns=feature_names_bg_high)
important_terms_bg_high = tfidf_scores_bg_high.mean().sort_values(ascending=False)[:30].index.tolist()


tfidf_scores_bg_high.columns

tfidf_shared_bg = TfidfVectorizer(
    max_features=100,
    stop_words="english",
    ngram_range=(2, 3),
    min_df=3,
    max_df=0.8
)
negative_df = reddit_df[reddit_df["Sentiment"] == "Negative"]
X_combined_bg = tfidf_shared_bg.fit_transform(negative_df["Combined_Text"])
y_combined_bg = (negative_df["Risk_Level"] == "High-Risk").astype(int)
chi2_scores_bg, p_values_bg = chi2(X_combined_bg, y_combined_bg)
feature_names_bg = tfidf_shared_bg.get_feature_names_out()
top_chi2_indices_bg = np.argsort(chi2_scores_bg)[::-1][:60]
top_chi2_words_bg = feature_names_bg[top_chi2_indices_bg]
significant_words_bg = [word for i, word in enumerate(top_chi2_words_bg) if p_values_bg[top_chi2_indices_bg[i]] < 0.25]
print("**Top High-Risk Indicator Words by Chi-Square Test (Filtered):**")
print(significant_words_bg)

high_risk_chi2_values = [
    (feature_names_bg[i], chi2_scores_bg[i])
    for i in top_chi2_indices_bg if p_values_bg[i] < 0.25
]
high_risk_chi2_values.sort(key=lambda x: x[1], reverse=True)
print("**Top 30 High-Risk Indicator Words with Chi-Square Scores:**")
for word, score in high_risk_chi2_values:
    print(f"{word}: {score:.4f}")

filtered_high_bg = [
    (word, score) for word, score in high_risk_chi2_values if score >= 4
]
filtered_high_bg

High_risk_crisis_terms = [word for word, score in filtered_high_bg]
print(High_risk_crisis_terms)

for term in High_risk_crisis_terms:
    if term not in High_risk_valid_words_dict:
        High_risk_valid_words_dict[term] = []
print(High_risk_valid_words_dict)

dictionary_keys_list = list(High_risk_valid_words_dict.keys())
print(dictionary_keys_list)

dictionary_keys_list = ['die', 'suicidal', 'kill', 'attempt', 'death', 'live fucking', 'fucking live', 'killed', 'commit', 'relapse','attempt', 'alive', 'suffer', 'overdose', 'kms', 'want die', 'suicidal thoughts', 'want kill', 'dont want live','panick attacks','panick attack' ]

def contains_high_risk_terms(text):
    text_lower = text.lower()
    return any(term in text_lower for term in dictionary_keys_list)
reddit_df_new = reddit_df.copy()
reddit_df_new["Risk_Adjusted"] = reddit_df_new["Risk_Level"]
mask = (reddit_df_new["Risk_Level"] == "Moderate Concern") & (reddit_df_new["Combined_Text"].apply(contains_high_risk_terms))
reddit_df_new.loc[mask, "Risk_Adjusted"] = "High-Risk"
print(f"Relabeled {mask.sum()} posts from 'Moderate Concern' to 'High-Risk'.")

relabeled_posts = reddit_df_new[mask]
print(relabeled_posts)

print(reddit_df_new["Risk_Adjusted"].value_counts())

from sklearn.decomposition import LatentDirichletAllocation
high_risk_df_new = reddit_df_new[reddit_df_new["Risk_Adjusted"] == "High-Risk"]
moderate_risk_df_new = reddit_df_new[reddit_df_new["Risk_Adjusted"] == "Moderate Concern"]
def apply_lda(texts, num_topics=5, max_features=1000):
    tfidf_vectorizer_new = TfidfVectorizer(
        max_features=max_features, stop_words="english", ngram_range=(1, 2), min_df=3, max_df=0.8
    )
    X_tfidf = tfidf_vectorizer_new.fit_transform(texts)
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(X_tfidf)
    feature_names_new = tfidf_vectorizer_new.get_feature_names_out()
    return lda, tfidf_vectorizer_new, feature_names_new
lda_high, tfidf_vectorizer_high, feature_names_new_high = apply_lda(high_risk_df_new["Combined_Text"])
lda_moderate, tfidf_vectorizer_moderate, feature_names_new_moderate = apply_lda(moderate_risk_df_new["Combined_Text"])
def get_lda_features(model, texts, vectorizer):
    X_tfidf = vectorizer.transform(texts)
    return model.transform(X_tfidf)
def print_topics(model, feature_names_new, title, num_words=10):
    print(f"\n **{title}**")
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names_new[i] for i in topic.argsort()[:-num_words - 1:-1]]
        print(f"**Topic {topic_idx + 1}:** {', '.join(top_words)}")
print_topics(lda_high, feature_names_new_high, "High-Risk Topics")
print_topics(lda_moderate, feature_names_new_moderate, "Moderate-Risk Topics")



X_lda_high = get_lda_features(lda_high, reddit_df_new[ "Combined_Text"], tfidf_vectorizer_high)
X_lda_moderate = get_lda_features(lda_moderate,  reddit_df_new["Combined_Text"], tfidf_vectorizer_moderate)
X_lda_combined = np.hstack((X_lda_high, X_lda_moderate))

import torch
print("Device:", torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print("CUDA available:", torch.cuda.is_available())
print("GPU name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU detected")


import torch
from transformers import BertTokenizer, BertModel
from sklearn.preprocessing import MultiLabelBinarizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")
bert_model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def get_bert_embeddings(texts, batch_size=32):
    bert_model.to(device)
    embeddings = []
    texts = [str(text) if isinstance(text, str) else "" for text in texts]
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
        inputs = {key: val.to(device) for key, val in inputs.items()}
        with torch.no_grad():
            outputs = bert_model(**inputs)
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move back to CPU
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)
X_bert = get_bert_embeddings(reddit_df_new[ "Combined_Text"])
high_risk_terms = ['die', 'suicidal', 'kill', 'attempt', 'death', 'live fucking', 'fucking live', 'killed', 'commit', 'relapse', 'alive',
                   'suffer', 'overdose', 'kms', 'hurt', 'want die', 'suicidal thoughts', 'want kill', 'want live', 'panic attacks', 'panic attack']
moderate_risk_terms = Moderate_Risk_Words_Final
def risk_feature(texts, terms):
    features = []
    for text in texts:
        features.append([1 if term in text.lower() else 0 for term in terms])
    return np.array(features)
X_high_risk = risk_feature(reddit_df_new["Combined_Text"], high_risk_terms)
X_moderate_risk = risk_feature(reddit_df_new["Combined_Text"], moderate_risk_terms)
X_final = np.hstack((X_bert,X_high_risk, X_moderate_risk, X_lda_combined))
print("Feature matrix shape:", X_final.shape)

reddit_df_new.columns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_labels = label_encoder.fit_transform(reddit_df_new["Risk_Adjusted"])
X_train, X_test, y_train, y_test = train_test_split(X_final, y_labels, test_size=0.2, random_state=42)
print(f"Training Data: {X_train.shape}, Testing Data: {X_test.shape}")

import torch.nn as nn
import torch.optim as optim
from sklearn.utils.class_weight import compute_class_weight
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)
y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)
class RiskClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(RiskClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.batch_norm2 = nn.BatchNorm1d(hidden_dim // 2)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)
        self.batch_norm3 = nn.BatchNorm1d(hidden_dim // 4)
        self.relu3 = nn.ReLU()
        self.dropout3 = nn.Dropout(0.3)
        self.fc4 = nn.Linear(hidden_dim // 4, hidden_dim // 8)
        self.batch_norm4 = nn.BatchNorm1d(hidden_dim // 8)
        self.relu4 = nn.ReLU()
        self.dropout4 = nn.Dropout(0.3)
        self.fc5 = nn.Linear(hidden_dim // 8, output_dim)
    def forward(self, x):
        x = self.fc1(x)
        x = self.batch_norm1(x)
        x = self.relu1(x)
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.batch_norm2(x)
        x = self.relu2(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        x = self.batch_norm3(x)
        x = self.relu3(x)
        x = self.dropout3(x)
        x = self.fc4(x)
        x = self.batch_norm4(x)
        x = self.relu4(x)
        x = self.dropout4(x)
        x = self.fc5(x)
        return x
input_dim = X_train.shape[1]
hidden_dim = 512
output_dim = len(label_encoder.classes_)
model = RiskClassifier(input_dim, hidden_dim, output_dim).to(device)
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=0.001)
print(model)


epochs = 20
batch_size = 32
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
print("Training complete!")

from sklearn.metrics import accuracy_score, classification_report
model.eval()
with torch.no_grad():
    y_pred_test = model(X_test_tensor)
    y_pred_labels = torch.argmax(y_pred_test, dim=1).cpu().numpy()
accuracy = accuracy_score(y_test, y_pred_labels)
print(f"Test Accuracy: {accuracy:.4f}")
print(classification_report(y_test, y_pred_labels, target_names=label_encoder.classes_))

import xgboost as xgb
from sklearn.metrics import classification_report
xgb_model = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(label_encoder.classes_),
    eval_metric="mlogloss",
    use_label_encoder=False,
    scale_pos_weight=[1, 1.1, 2],
    learning_rate=0.05,
    max_depth=6,
    n_estimators=500,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertModel
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
class RedditDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return {"input_ids": input_ids, "attention_mask": attention_mask, "label": label}
train_texts, test_texts, y_train, y_test = train_test_split(
    reddit_df_new["Combined_Text"].tolist(), y_labels, test_size=0.2, random_state=42
)
train_dataset = RedditDataset(train_texts, y_train, tokenizer)
test_dataset = RedditDataset(test_texts, y_test, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
class DistilBERTClassifier(nn.Module):
    def __init__(self, num_classes):
        super(DistilBERTClassifier, self).__init__()
        self.distilbert = DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(768, num_classes)
    def forward(self, input_ids, attention_mask):
        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        x = self.dropout(pooled_output)
        x = self.fc(x)
        return x
num_classes = len(label_encoder.classes_)
model = DistilBERTClassifier(num_classes).to(device)
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.AdamW(model.parameters(), lr=2e-5)
def train(model, dataloader, criterion, optimizer, device, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        correct, total = 0, 0
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            correct += (outputs.argmax(1) == labels).sum().item()
            total += labels.size(0)
        accuracy = correct / total
        print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {accuracy:.4f}")
train(model, train_loader, criterion, optimizer, device, epochs=5)
torch.save(model, "distilbert_risk_classifier_new.pth")


from sklearn.metrics import accuracy_score, classification_report
def evaluate(model, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)
            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    accuracy = accuracy_score(all_labels, all_preds)
    print(f"\nTest Accuracy: {accuracy:.4f}")
    print("\nClassification Report:\n", classification_report(all_labels, all_preds))
evaluate(model, test_loader, device)
